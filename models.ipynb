{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e7eedd-b291-42de-8da8-94653c1d0232",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    " LLMs, chat models, and embedding models from different providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12d4789f-eeb2-48ed-894f-f4b530ee3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c608f-58b2-4ca0-ac4d-ec97ab508f9f",
   "metadata": {},
   "source": [
    "## LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12925bd4-27fd-4a9b-9227-b377db1c3d17",
   "metadata": {},
   "source": [
    "- OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a03b792-edfa-404b-8cf5-f9238852cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_API_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "try:\n",
    "    result = llm.invoke(\"Explain LLMs in simple terms.\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a427429-c5ba-4e21-a31f-ce728c2fc637",
   "metadata": {},
   "source": [
    "- Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5426d34e-c48f-4b9c-b275-a9b00aa5ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import AnthropicLLM\n",
    "llm =  AnthropicLLM(model=\"claude-3-sonnet-20240229\", temperature=0.7)\n",
    "try:\n",
    "    result = llm.invoke(\"Explain LLMs in simple terms.\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62e7e69-98d5-420c-beeb-8ed2bd9e9433",
   "metadata": {},
   "source": [
    "- Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c55a7-6efd-41b1-a715-3f8ad72dd595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I don’t know what you mean by “simple”. I am not a mathematician or an engineer, and I am not a lawyer. I am a retired teacher.\n",
      "I am not a lawyer, but I am a teacher. I am not a mathematician or an engineer, but I am a retired teacher. I am not a mathematician or an engineer, but I am a retired teacher.\n",
      "I am not a mathematician or an engineer, but I am a retired teacher. I am not a mathematician or an engineer, but I am a retired teacher.\n",
      "I am not a mathematician or an engineer, but\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "\n",
    "repo_id =\"tiiuae/falcon-rw-1b\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = repo_id,\n",
    "     task=\"text-generation\",\n",
    "     temperature=0.5,\n",
    "    max_new_tokens=128 \n",
    ")\n",
    "try:\n",
    "    result = llm.invoke(\"Explain LLMs in simple terms.\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa608b8-6205-4414-9fb8-aa56dc695892",
   "metadata": {},
   "source": [
    "The free serverless API lets you implement solutions and iterate in no time, but it may be rate limited for heavy use cases, since the loads are shared with other requests.\n",
    "For enterprise workloads, the best is to use Inference Endpoints - Dedicated. This gives access to a fully managed infrastructure that offer more flexibility and speed. These resoucres come with continuous support and uptime guarantees, as well as options like AutoScaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc854f5-2a28-4ce0-8e90-e3217e7cb008",
   "metadata": {},
   "source": [
    "- Function calling\n",
    "- \n",
    "The function calliong lets LLMs act more like agents, deciding when to invoke tools (APIs, functions, calculators, databases, etc.) based on natural language queries—great for use cases like:\n",
    "    - Calling APIs (weather, flight, calendar)\n",
    "    - Triggering internal business logic\n",
    "    - Making structured outputs that are easier to parse downstream\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd5cd64d-72a1-4f88-8347-aa575fed15dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "San Francisco is a beautiful city, with a temperate climate. The average temperature is around 50°F (10°C) in the summer and around 30°F (0°C) in the winter. The average annual rainfall is around 12 inches (30 cm).\n",
      "The average annual rainfall is around 12 inches (30 cm).\n",
      "What's the best time to visit San Francisco?\n",
      "The best time to visit San Francisco is between May and October, when the weather is warm and sunny.\n",
      "The best time to visit San Francisco is between May and October, when the weather is warm and sunny.\n",
      "What's the\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class WeatherSearch(BaseModel):\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    unit: str = Field(\"fahrenheit\", description=\"The temperature unit to use\")\n",
    "\n",
    "llm = llm.bind(functions=[WeatherSearch.schema()])\n",
    "\n",
    "\n",
    "try:\n",
    "    response = llm.invoke(\"What's the weather like in San Francisco?\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73682a35-3551-4f21-a0fc-23325195529a",
   "metadata": {},
   "source": [
    "## Chat Models \n",
    "In LangChain, an LLM is used for standard single-turn text completion tasks—just one prompt in, one response out. A ChatModel, on the other hand, is designed for multi-turn conversations and takes a sequence of messages (like from a user or assistant) as input. Use LLM for simple prompts and ChatModel when you need structured back-and-forth dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17498a35-93b8-41fa-a3b7-08395b21ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_API_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "# Open AI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Explain the concept of recursion in programming.\")\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = chat.invoke(messages)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac5842b-bcaf-41bb-ab4a-f0c833117bd8",
   "metadata": {},
   "source": [
    "- Streaming responses\n",
    "- \n",
    "Streaming responses in LangChain means that the model generates and returns the output in real-time, as it is produced, instead of waiting for the entire response to be completed before sending it back. This is useful when you're working with large responses or when you want to display results progressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f726f67b-1960-48c9-87ee-0b8067119ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_API_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = chat.invoke(\"Explain LLMs in simple terms.\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# PS: not supported for  HuggingFaceEndpoint (ref: https://github.com/langchain-ai/langchain/issues/7785)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f158e4e-170c-456c-a3a5-c6996f0aeee9",
   "metadata": {},
   "source": [
    "## Text Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "792f7d30-3e9a-462c-bfdc-bf38bd2d3a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00012550536484923214, 0.05915382504463196, 0.01599189266562462, -0.028340505436062813, 0.006711236666887999, -0.005119623150676489, 0.13805529475212097, 0.013968885876238346, -0.04378935322165489, 0.023291287943720818]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "text = \"Write a short poem about programming.\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "try:\n",
    "    embedding_vector = embeddings.embed_query(text)\n",
    "    print(embedding_vector[:10])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28442d-e077-4bd6-b358-492258d6407e",
   "metadata": {},
   "source": [
    "## Create your own Model based on a client inference API\n",
    "\n",
    "in this case we use Groq inference: https://groq.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20440f0-b193-43c4-97a2-b0c9cffcc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"llama3-8b-8192\"\n",
    "DEFAULT_TEMPERATURE = 0.3\n",
    "DEFAULT_MAX_TOKENS = 512\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from groq import Groq\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class GroqLLM(LLM):\n",
    "    \"\"\"Custom LLM wrapper for Groq API.\"\"\"\n",
    "\n",
    "    model: str = Field(default=DEFAULT_MODEL)\n",
    "    temperature: float = Field(default=DEFAULT_TEMPERATURE)\n",
    "    max_tokens: int = Field(default=DEFAULT_MAX_TOKENS)\n",
    "    api_key: Optional[str] = Field(default=None)\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Call the Groq API with the given prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to send to the API\n",
    "            stop: Optional list of stop sequences\n",
    "\n",
    "        Returns:\n",
    "            The response from the API\n",
    "        \"\"\"\n",
    "        client = Groq(api_key=self.api_key or GROQ_API_KEY)\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return the type of LLM.\"\"\"\n",
    "        return \"groq-llm\"\n",
    "\n",
    "llm = GroqLLM()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
