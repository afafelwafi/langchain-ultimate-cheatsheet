{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82c4209-cc30-4c2c-8a95-3fecc1cd25b1",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738346c-9866-426a-94b9-ce2b303656bb",
   "metadata": {},
   "source": [
    "- Prepare a Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06b001c8-63a0-411a-bd1c-b301267b506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4468017-32c5-4374-a01d-4bea2f328be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# =================== Section to change according to your choice of APIs you have access to===============\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# llm\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=128  # Adjust max_new_tokens for generation\n",
    ")\n",
    "\n",
    "# Set up ChatHuggingFace\n",
    "chatllm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e45abd-cdd6-43b6-a90e-3dc6d9ada9ea",
   "metadata": {},
   "source": [
    "##  LangChain Expression Language (LCEL) - which is LangChain's declarative way to compose chains using the pipe (|) (previously : LLMChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6658a09-75b0-4044-bf89-aca450764b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What are 5 cool names for a {product}?\"\n",
    ")\n",
    "\n",
    "chain = prompt | chatllm\n",
    "result = chain.invoke({\"product\": \"AI-powered toothbrush\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc44fb6-cd3b-4a9c-91d7-9f1b4e526b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. BrushBot: A smart, AI-powered toothbrush that uses advanced technology to ensure optimal oral hygiene.\n",
      "\n",
      "2. SmartSweep: An AI toothbrush that adapts to your brushing habits, providing real-time feedback and guidance for a thorough clean.\n",
      "\n",
      "3. PolarPerio: A futuristic, AI-driven toothbrush that monitors your dental health and provides personalized care recommendations.\n",
      "\n",
      "4. OralOracle: An intelligent toothbrush that provides predictions and preventive care for your oral health with the help of its AI brain.\n",
      "\n",
      "5. GumGuardian: An AI-powered toothbrush designed to protect and improve gum health, using smart sensors to monitor and adjust brushing patterns accordingly.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46521d-5d9d-48e9-b7fd-835888d37f3f",
   "metadata": {},
   "source": [
    "## Sequential Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034cab4-d03f-4a38-bb0a-3b167dff5e14",
   "metadata": {},
   "source": [
    "- Simple Sequential Chain (single input/output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "507c8821-58a1-495e-ad8f-4b87b0f077c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Title: \"Stardust Odyssey: The Last Frontier\"\n",
      "\n",
      "In the year 2135, humanity has reached the pinnacle of space exploration, with colonies on Mars and the Moon, and regular missions to the asteroid belt. However, a mysterious signal is detected emanating from a distant star system, sparking a new era of exploration.\n",
      "\n",
      "Dr. Amelia Hart, a brilliant astrophysicist, decodes the signal and discovers it's an invitation from an alien civilization. The message reveals that the aliens have a powerful technology that could solve\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m Earth's most pressing problems, but they're located in a remote and dangerous region of space known as the Last Frontier.\n",
      "\n",
      "The United Earth Government (UEG) assembles an elite team of astronauts, scientists, and military personnel, led by Captain John Reynolds, to embark on a mission to the Last Frontier. The team includes Dr. Hart, who's been chosen for her expertise in decoding the alien language, and Lt. Commander James \"Sully\" Sullivan, a skilled pilot with a reputation for being reckless.\n",
      "\n",
      "The journey to the Last Front\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain, LLMChain\n",
    "\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a short story idea about {topic}.\"\n",
    ")\n",
    "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"story_idea\"],\n",
    "    template=\"Create a title for this story: {story_idea}\"\n",
    ")\n",
    "second_chain = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "simple_sequential_chain = SimpleSequentialChain(\n",
    "    chains=[first_chain, second_chain],\n",
    "    verbose=True\n",
    ")\n",
    "result = simple_sequential_chain.invoke(\"space exploration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a32c1eda-ca27-45fe-b7fc-7d94e3341c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With LCEL\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "\n",
    "\n",
    "template1 = ChatPromptTemplate.from_template(\"Write a story about {topic}\")\n",
    "template2 = ChatPromptTemplate.from_template(\"Create a title for this story: {story}\")\n",
    "\n",
    "chain1 = template1 | llm | StrOutputParser()\n",
    "chain2 = template2 | llm | StrOutputParser()\n",
    "\n",
    "final_chain = {\n",
    "    \"story\": chain1,\n",
    "    \"topic\": RunnablePassthrough()\n",
    "} | chain2\n",
    "\n",
    "result = final_chain.invoke(\"a brave astronaut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebcdc1-e08f-4e5e-9088-840599f76ef0",
   "metadata": {},
   "source": [
    "- Sequential Chain (multiple inputs/outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05722667-80a1-4623-9bee-122a2f6f00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a short story idea about {topic}.\"\n",
    ")\n",
    "first_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=first_prompt,\n",
    "    output_key=\"story_idea\"\n",
    ")\n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"story_idea\", \"genre\"],\n",
    "    template=\"Create a {genre} title for this story: {story_idea}\"\n",
    ")\n",
    "second_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=second_prompt,\n",
    "    output_key=\"title\"\n",
    ")\n",
    "\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[first_chain, second_chain],\n",
    "    input_variables=[\"topic\", \"genre\"],\n",
    "    output_variables=[\"story_idea\", \"title\"],\n",
    "    verbose=True\n",
    ")\n",
    "result = sequential_chain.invoke({\n",
    "    \"topic\": \"space exploration\",\n",
    "    \"genre\": \"science fiction\"\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e0ae77-99a3-43d4-a07a-ef8b05c36557",
   "metadata": {},
   "source": [
    "## Router Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591cdff-1e61-45f5-ae5d-4361f0435959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "physics_template = \"\"\"You are a physics expert. Answer this physics question:\n",
    "{query}\"\"\"\n",
    "math_template = \"\"\"You are a math expert. Answer this math question:\n",
    "{query}\"\"\"\n",
    "history_template = \"\"\"You are a history expert. Answer this history question:\n",
    "{query}\"\"\"\n",
    "\n",
    "physics_prompt = ChatPromptTemplate.from_template(physics_template)\n",
    "math_prompt = ChatPromptTemplate.from_template(math_template)\n",
    "history_prompt = ChatPromptTemplate.from_template(history_template)\n",
    "\n",
    "destination_chains = {\n",
    "    \"physics\": physics_prompt | llm,\n",
    "    \"math\": math_prompt | llm,\n",
    "    \"history\": history_prompt | llm\n",
    "}\n",
    "\n",
    "router_template = \"\"\"Given a query, select the category it belongs to:\n",
    "1. Physics: Questions about physics, forces, energy, etc.\n",
    "2. Math: Questions about mathematics, calculations, etc.\n",
    "3. History: Questions about historical events, periods, people, etc.\n",
    "\n",
    "Query: {query}\n",
    "Category:\"\"\"\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(\n",
    "    llm=llm,\n",
    "    prompt=router_prompt\n",
    ")\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=chainllm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"What is the formula for kinetic energy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353415a-0ec4-4e3b-95ed-84b8963f3034",
   "metadata": {},
   "source": [
    "## Question Answering Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93d6e2-5d45-4e88-b836-99e72332a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load document\n",
    "loader = TextLoader(\"files/data.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Create chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Other options: \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke(\"What is the main topic of the document?\")\n",
    "\n",
    "# With LCEL\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = qa_chain.invoke(\"What is the main topic of the document?\")\n",
    "\n",
    "# With source documents\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "response = qa_with_sources.invoke(\"What is the main topic of the document?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
