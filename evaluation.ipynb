{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abd0538-b89a-49ee-b1b0-d73da003da72",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Evaluation in LangChain helps assess how good LLM outputs are. It supports:\n",
    "- Criteria-based checks (e.g. correctness, relevance)\n",
    "- QA-style evaluations (question vs answer quality)\n",
    "- String match or semantic similarity\n",
    "- Can use LLMs themselves to grade responses\n",
    "- Useful for testing prompts, chains, or full agents\n",
    "\n",
    "It’s a key part of making sure your LLM pipeline works well. [ref](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5255ce2-6c6b-460c-ab81-1a475999ede7",
   "metadata": {},
   "source": [
    "- Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d5102a2-c81a-465b-9688-49aa21ad1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain.evaluation import load_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90ba0c-6afa-434c-a37d-c31045979cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Section to change according to your choice of APIs you have access to===============\n",
    "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
    "\n",
    "# llm\n",
    "repo_id =\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = repo_id,\n",
    "     task=\"text-generation\",\n",
    "     temperature=0.5,\n",
    "    max_new_tokens=128 \n",
    ")\n",
    "\n",
    "# embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "text = \"Write a short poem about programming.\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7de5c-743f-477d-866e-7df26e7698f0",
   "metadata": {},
   "source": [
    "- Using built-in criteria\n",
    "\n",
    "Criteria Evaluator is a tool to evaluate LLM responses based on human-aligned judgment categories such as helpfulness, correctness, relevance, etc. It helps automate the process of judging if a model's response is good, accurate, clear, or useful.\n",
    "\n",
    " Common Criteria You Can Use:\n",
    "- helpfulness:\tIs the answer useful and informative?\n",
    "- correctness:\tIs the answer factually accurate?\n",
    "- conciseness:\tIs the response short and to the point without fluff?\n",
    "- relevance:\tDoes it address the user’s question or prompt directly?\n",
    "- coherence:\tIs the answer logically structured and easy to follow?\n",
    "- harmlessness:\tDoes it avoid toxic or harmful content?\n",
    "- complexity:\tIs the output appropriately complex for the task or audience?\n",
    "- truthfulness:\tDoes it avoid hallucinating or making up false information?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb8861b7-dcef-45d0-97c2-904ede016f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_evaluator = load_evaluator(\"criteria\", criteria=\"helpfulness\",llm=llm)\n",
    "eval_result = criteria_evaluator.evaluate_strings(\n",
    "    prediction=\"Mitochondria are the powerhouse of the cell\",\n",
    "    reference=None, # Optional reference answer if you have one\n",
    "    input=\"What are mitochondria?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc65bd9-8bd2-494b-9528-993dd309ec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': \"Reasoning:\\n1. helpfulness: The submission provides a concise and accurate definition of mitochondria, which is helpful for someone who is asking about the topic. It is insightful as it gives a general understanding of mitochondria's role in the cell. The answer is appropriate for the question asked.\\n\\nFinal answer: Y\\n\\nFinal answer:\", 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54e3b5-e752-4456-bfe0-2b36124f6eda",
   "metadata": {},
   "source": [
    "- QA evaluator\n",
    "  \n",
    "The QA Evaluator in LangChain is used to evaluate how well an LLM-generated answer matches a reference (ground-truth) answer, usually in a question-answering (QA) context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "117f8e73-b001-4ae4-8d93-8816cb7a718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "qa_evaluator = load_evaluator(\"qa\",llm=llm)\n",
    "qa_result = qa_evaluator.evaluate_strings(\n",
    "    prediction=\"The Earth is approximately 4.5 billion years old\",\n",
    "    reference=\"\",\n",
    "    input=\"How old is the Earth?\"\n",
    ")\n",
    "print(qa_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ab43f-fc0a-4269-84cb-5296c4bb5939",
   "metadata": {},
   "source": [
    "- String evaluator\n",
    " \n",
    "It loads a basic evaluator that compares the model’s output string to a reference string using simple heuristics like:Exact match, String similarity,Partial overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d54f1f5-357b-46eb-bce8-c53d03060300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.15888888888888886}\n"
     ]
    }
   ],
   "source": [
    "string_evaluator = load_evaluator(\"string_distance\",llm=llm)\n",
    "string_result = string_evaluator.evaluate_strings(\n",
    "    prediction=\"Apples are red fruits that grow on trees\",\n",
    "    reference=\"Apples are fruits that grow on trees and are often red, green, or yellow\"\n",
    ")\n",
    "print(string_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e515e16-a0da-41dc-b7c3-1f147bea6fd1",
   "metadata": {},
   "source": [
    "- Labeled Criteria\n",
    "  \n",
    "It's an evaluator that compares:A prediction (e.g. a model’s output) Against a reference labelBased on one or more criteria (e.g. truthfulness, relevance, coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6c75f13-feab-4d3f-9984-b4208011cdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': '1. Accuracy:\\n- The submission correctly states that Paris is the capital of France.\\n- The population of Paris is approximately 2.1 million people, which is a factually accurate figure as of the latest available data.\\n\\n2. Conciseness:\\n- The response provides two pieces of information: the capital of France and its population.\\n- The response is relatively brief, but it could be made even more concise by omitting the population figure.\\n\\n3. Helpfulness:\\n- The response accurately answers the question about the capital of France.\\n- The population figure', 'value': '- The population figure', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "llm_evaluator = load_evaluator(\"labeled_criteria\", \n",
    "                              criteria={\n",
    "                                  \"accuracy\": \"Is the information factually accurate?\",\n",
    "                                  \"conciseness\": \"Is the response concise and to the point?\",\n",
    "                                  \"helpfulness\": \"Is the response helpful for the user?\"\n",
    "                              },\n",
    "                              llm=llm)\n",
    "\n",
    "llm_result = llm_evaluator.evaluate_strings(\n",
    "    prediction=\"Paris is the capital of France and has a population of about 2.1 million people.\",\n",
    "    input=\"What is the capital of France?\",\n",
    "    reference=\"\",\n",
    ")\n",
    "print(llm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5037d-28fd-40f9-8194-2424de2068ac",
   "metadata": {},
   "source": [
    "- Embedding-based evaluators\n",
    "  \n",
    "Embedding-based evaluators compare outputs using vector similarity, not exact words. They measure how semantically close two texts are — great for evaluating meaning even when wording differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ddfb433-2567-445d-91c3-2995a5ad486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.036108852493081045}\n"
     ]
    }
   ],
   "source": [
    " from langchain.evaluation.embedding_distance import EmbeddingDistance\n",
    "\n",
    "embedding_evaluator = load_evaluator(\n",
    "    \"embedding_distance\", \n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "distance_result = embedding_evaluator.evaluate_strings(\n",
    "    prediction=\"The sky is blue because of Rayleigh scattering\",\n",
    "    reference=\"The sky appears blue due to a phenomenon called Rayleigh scattering\"\n",
    ")\n",
    "print(distance_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
