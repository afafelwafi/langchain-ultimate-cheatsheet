{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0eadcdc-713f-43db-b3ff-8070eebfde30",
   "metadata": {},
   "source": [
    "# Integration\n",
    "Langchain offers a variety of plugging in external tools and systems — like APIs, databases, vector stores, file loaders, or LLM providers — directly into your LLM workflow. It makes your agent or app smarter, more interactive, and connected to real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d829b",
   "metadata": {},
   "source": [
    "- Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b18a532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a branch of computing that utilizes the principles of quantum mechanics to perform operations on data. Unlike classical computers that use bits (0s or 1s) for processing information, quantum computers use quantum bits, or qubits. A qubit can exist in multiple states at once—a property called superposition—and can be entangled with other qubits, meaning the state of one qubit can be dependent on the state of another, no matter the distance between them.\n",
      "\n",
      "Two fundamental ideas in quantum computing are quantum parallelism and quantum tunneling. Quantum parallelism allows a quantum computer to perform many calculations simultaneously due to the superposition property of qubits. Instead of following a sequential path like a classical computer, a quantum computer can exist in many states at once, each representing a possible solution. Quantum tunneling, on the other hand, refers to the ability of particles to pass through barriers that they wouldn't be able to in classical physics, which can significantly speed up certain operations.\n",
      "\n",
      "One crucial algorithm in quantum computing is Shor's algorithm, which can factor large numbers exponentially faster than classical algorithms. This poses a potential threat to the security of RSA encryption, one of the cornerstones of internet security.\n",
      "\n",
      "The development of practical quantum computing technology is still in its early stages. Quantum decoherence, errors caused by interactions between qubits and their environment, and difficulty in scaling up the number of qubits are some major challenges to overcome. However, progress is being made, with companies like IBM, Google, and startups around the world actively working on building quantum computers and applications. Quantum computing has the potential to revolutionize various fields, such as cryptography, drug discovery, and artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "# =================== Section to change according to your choice of APIs you have access to===============\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# llm\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",  # Ensure the correct task type\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=128  # Adjust max_new_tokens for generation\n",
    ")\n",
    "\n",
    "# Set up ChatHuggingFace\n",
    "chatllm = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "# Test with a prompt\n",
    "try:\n",
    "    response = chatllm.invoke(\"Explain the concept of quantum computing.\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f0622-b8ee-45ad-8cb5-0b5a1368e931",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa5c37-0ed4-4f0f-82cc-6ad9c28d1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Text completion models\n",
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "# Chat models\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40155bac-e4d4-4d39-8766-c910533464ff",
   "metadata": {},
   "source": [
    "## Anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2d260-6b66-472e-9dfa-e7025ec34c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Chat models\n",
    "claude = ChatAnthropic(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    temperature=0.7,\n",
    "    max_tokens_to_sample=1000\n",
    ")\n",
    "\n",
    "response = claude.invoke(\"Explain the concept of quantum entanglement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d441acf-a54b-41e8-ac5f-f803888ab5ff",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93cddb-9cd0-445f-b46a-49dc7834d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
    "\n",
    "# LLM via Inference API\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"https://api-inference.huggingface.co/models/google/flan-t5-xxl\",\n",
    "    task=\"text2text-generation\"\n",
    ")\n",
    "\n",
    "# Local pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Embeddings\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474044b0-ce3e-4374-9226-f15f97946858",
   "metadata": {},
   "source": [
    "## Google Cloud Vertex AI\n",
    "\n",
    "You can use the GOOGLE_APPLICATION_CREDENTIALS environment variable to provide the location of a JSON credentials file. This file will store the credentials loaded by google-auth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df78b3f-801c-4966-9eef-34a57da36b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI, ChatVertexAI, VertexAIEmbeddings\n",
    "\n",
    "# Text models\n",
    "vertex_llm = VertexAI(\n",
    "    model_name=\"text-bison@002\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    project=\"your-project-id\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# Chat models\n",
    "vertex_chat = ChatVertexAI(\n",
    "    model_name=\"chat-bison@002\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.7,\n",
    "    project=\"your-project-id\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# Embeddings\n",
    "vertex_embeddings = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@001\",\n",
    "    project=\"your-project-id\",\n",
    "    location=\"us-central1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7217804c-0b83-40d4-ab68-9b37eee2ed7b",
   "metadata": {},
   "source": [
    "## AWS Bedrock\n",
    "\n",
    "The he name of the profile in the ~/.aws/credentials or ~/.aws/config files, which has either access keys or role information specified. If not specified, the default credential profile or, if on an EC2 instance, credentials from IMDS will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1937d6-5594-4f91-9b01-aec42157beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockLLM, BedrockEmbeddings\n",
    "\n",
    "# Text models\n",
    "bedrock_llm = BedrockLLM(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    region_name=\"us-west-2\",\n",
    "    credentials_profile_name=\"default\"\n",
    ")\n",
    "\n",
    "# Embeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\",\n",
    "    region_name=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# With custom model parameters\n",
    "from langchain_aws import BedrockChat\n",
    "\n",
    "bedrock_chat = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c57ac-ecf9-4ab6-8f73-b190d22ce90c",
   "metadata": {},
   "source": [
    "## Templates and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f6d81-622d-4c88-b08a-7cc2381a3eef",
   "metadata": {},
   "source": [
    "### RAG (Retrieval-Augmented Generation) Pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "225709d3-2f7b-42bc-9569-c66ce057ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle and is part of a rich ecosystem of tools that integrate with it. LangChain provides tutorials, conceptual guides, how-to guides, API reference, and integrations to help in the development process. LangGraph, a part of the LangChain ecosystem, is a tool for building stateful, multi-actor applications with LLMs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import  ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 1. Load documents\n",
    "loader = WebBaseLoader(\"https://docs.langchain.com/docs/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Create embeddings and vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 4. Create prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# 5. Create RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chatllm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 7. Run the chain\n",
    "response = rag_chain.invoke(\"What is LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0faf7e6-e24f-4913-8bc2-0341de2b6167",
   "metadata": {},
   "source": [
    "### Agent with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35277b27-a79b-496b-a639-3f959f2006ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, create_openai_functions_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# 1. Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=lambda x: f\"Search results for: {x}\",\n",
    "        description=\"Useful for searching information\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=lambda x: eval(x),\n",
    "        description=\"Useful for performing calculations\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 2. Setup memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 3. Create prompt with memory\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant with access to tools.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# 4. Create the agent\n",
    "agent = create_openai_functions_agent(chatllm, tools, prompt)\n",
    "\n",
    "# 5. Create the executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 6. Run the agent\n",
    "response = agent_executor.invoke({\"input\": \"What's the capital of France?\"})\n",
    "follow_up = agent_executor.invoke({\"input\": \"What's the population of its capital?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64272df3-9f88-4f7e-a2d3-eb18292ac9d4",
   "metadata": {},
   "source": [
    "### Custom Tool Creation Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe222c71-7c4c-43d5-bfe6-55d958feb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import StructuredTool, tool, Tool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from typing import Optional, List\n",
    "\n",
    "# 1. Simple function-based tool with @tool decorator\n",
    "@tool\n",
    "def search_database(query: str) -> str:\n",
    "    \"\"\"Search the database for information.\"\"\"\n",
    "    # Implement your database search logic here\n",
    "    return f\"Database results for: {query}\"\n",
    "\n",
    "# 2. Tool with Pydantic schema\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(..., description=\"City and country or ZIP code\")\n",
    "    unit: str = Field(\"celsius\", description=\"Temperature unit (celsius/fahrenheit)\")\n",
    "    \n",
    "@tool(args_schema=WeatherInput)\n",
    "def get_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Get the current weather for a location.\"\"\"\n",
    "    # Implement weather API call here\n",
    "    return f\"Weather in {location} is sunny and 22°{unit[0].upper()}\"\n",
    "\n",
    "# 4. Create agent with tools\n",
    "tools = [search_database, get_weather]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    chatllm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c425d4-2929-4088-a688-bab69a4812af",
   "metadata": {},
   "source": [
    "### LangSmith Integration\n",
    "LangSmith is a tool within the LangChain ecosystem designed to help you manage, monitor, and evaluate the performance of your language model (LLM) applications. It provides features for debugging, tracking, and analyzing the behavior of models during real-world use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cac1b-dedd-468a-932d-71b96d8d6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import RunEvaluator\n",
    "\n",
    "# Create LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Normal LangChain operations will now be traced automatically\n",
    "result = llm.invoke(\"Tell me a joke\")\n",
    "\n",
    "# Log feedback after execution\n",
    "# Generate a valid run_id\n",
    "run_id = str(uuid.uuid4())\n",
    "client.create_feedback(\n",
    "    run_id=run_id,\n",
    "    key=\"helpfulness\",\n",
    "    score=0.9,\n",
    "    comment=\"Very helpful response\"\n",
    ")\n",
    "\n",
    "# Run evaluations\n",
    "# Assuming you have already configured your evaluation criteria and runs\n",
    "evaluator = RunEvaluator()\n",
    "\n",
    "# Evaluate the runs based on your dataset and experiment\n",
    "evaluation_results = evaluator.evaluate_runs(\n",
    "    dataset_name=\"your_dataset\",  # Ensure you have a dataset\n",
    "    experiment_name=\"your_experiment\"  # Ensure you have an experiment\n",
    ")\n",
    "\n",
    "print(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1843b-98bd-47ac-901c-39e499760ee5",
   "metadata": {},
   "source": [
    "### Custom Chain with LCEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16ec70ba-3420-4d04-b51b-3608f975c32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest information, as of April 23, 2024, includes the following:\n",
      "1. Result 1\n",
      "2. Result 2\n",
      "3. Result 3\n",
      "\n",
      "Please note that the time and date of the information are crucial for understanding the context. If more specific details are needed, it would be beneficial to know the topic or field to which these results pertain, as the meaning of \"latest\" can vary depending on the context. For example, \"latest\" in the field of technology might mean something different from \"latest\" in the field of politics.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from typing import Dict, Any\n",
    "\n",
    "# 1. Define component functions\n",
    "def fetch_data(query: str) -> Dict[str, Any]:\n",
    "    # Fetch relevant data for the query\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"timestamp\": \"2024-04-23\",\n",
    "        \"results\": [\"Result 1\", \"Result 2\", \"Result 3\"]\n",
    "    }\n",
    "\n",
    "def format_results(results: List[str]) -> str:\n",
    "    return \"\\n\".join([f\"- {item}\" for item in results])\n",
    "\n",
    "# 2. Create a custom data processing chain\n",
    "data_chain = RunnableLambda(fetch_data)\n",
    "\n",
    "# 3. Create a prompt template\n",
    "template = \"\"\"Based on the following information:\n",
    "Query: {query}\n",
    "Timestamp: {timestamp}\n",
    "Results:\n",
    "{formatted_results}\n",
    "\n",
    "Provide a comprehensive answer to the query.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Build the full chain\n",
    "chain = (\n",
    "    RunnablePassthrough() \n",
    "    | data_chain \n",
    "    | {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"timestamp\": lambda x: x[\"timestamp\"],\n",
    "        \"formatted_results\": lambda x: format_results(x[\"results\"])\n",
    "    }\n",
    "    | prompt\n",
    "    | chatllm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 5. Run the chain\n",
    "response = chain.invoke(\"What's the latest information?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0204f04-a680-4199-9495-144cda9a5463",
   "metadata": {},
   "source": [
    "### Error Handling and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7eaaa4f-edea-4ffd-a2e9-5ae3fff0e29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris. It is one of the most significant cities in the world for art, fashion, gastronomy, and culture and is known for its iconic landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. Paris has long been a leading global city and a major center for academics, arts, commerce, and politics.', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=82, prompt_tokens=10, total_tokens=92), 'model': '', 'finish_reason': 'stop'}, id='run-cc38cb3a-7852-4f9a-8086-bf7af6a2ae1d-0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables.retry import RunnableRetry\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "import tenacity\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Assuming 'chatllm' is defined earlier as an instance of an LLM\n",
    "\n",
    "# 1. Simple retry with exponential backoff\n",
    "llm_with_retry = chatllm.with_retry(\n",
    "    retry_if_exception_type=(ValueError, ConnectionError),\n",
    "    # max_retries=5,  # Set retry limit\n",
    "    # backoff_factor=2,  # Exponential backoff factor\n",
    "    # wait=tenacity.wait_exponential(multiplier=1, min=4, max=10)  # Exponential wait\n",
    ")\n",
    "\n",
    "# 2. Advanced retry with RunnableRetry\n",
    "retry_config = RunnableRetry(\n",
    "    bound=llm_with_retry,\n",
    "    retry_if_exception_type=(ValueError, ConnectionError),\n",
    "    max_retries=5,\n",
    "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=10)\n",
    ")\n",
    "\n",
    "# Wrap the LLM with advanced retry configuration\n",
    "llm_with_advanced_retry = retry_config\n",
    "\n",
    "# 3. Try/except pattern for more control\n",
    "def safe_parse_json(text):\n",
    "    try:\n",
    "        import json\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON\", \"text\": text}\n",
    "\n",
    "# 4. Fallback chains\n",
    "def is_complex_question(input):\n",
    "    # Determine if a question requires advanced reasoning\n",
    "    return len(input.split()) > 10\n",
    "\n",
    "chain = RunnableBranch(\n",
    "    (is_complex_question, chatllm),  # Use model 1 for complex questions\n",
    "    chatllm# Use same model (we could have changed to another one ) for simpler questions\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is the capital of France?\")  # Simple question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8510e5d-3184-4d32-b505-662dc3748eaa",
   "metadata": {},
   "source": [
    "### Streaming Response Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08286970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import asyncio\n",
    "\n",
    "# 1. Basic streaming with callbacks\n",
    "# =================== Section to change according to your choice of APIs you have access to===============\n",
    "streaming_llm  = ChatHuggingFace(llm=llm,streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "streaming_llm.invoke(\"Write a short story about space exploration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e1cf8-1465-4a2a-8699-23b4ed30fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import asyncio\n",
    "\n",
    "# 1. Basic streaming with callbacks\n",
    "# =================== Section to change according to your choice of APIs you have access to===============\n",
    "streaming_llm  = ChatHuggingFace(llm=llm,streaming=True, callbacks=[StreamingStdOutCallbackHandler])\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "streaming_llm.invoke(\"Write a short story about space exploration\")\n",
    "\n",
    "# 2. Custom streaming handler\n",
    "class CustomStreamingHandler(StreamingStdOutCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.text = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "handler = CustomStreamingHandler()\n",
    "\n",
    "# =================== Section to change according to your choice of APIs you have access to===============\n",
    "chatllm = ChatHuggingFace(llm=llm,streaming=True, callbacks=[handler])\n",
    "#=========================================================================================================\n",
    "\n",
    "llm.invoke(\"Write a poem about AI\")\n",
    "\n",
    "# 3. Async streaming with LCEL (Jupyter-friendly)\n",
    "async def generate_stream():\n",
    "    prompt = ChatPromptTemplate.from_template(\"Write a {length} story about {topic}\")\n",
    "    chain = prompt | chatllm | StrOutputParser()\n",
    "\n",
    "    async for chunk in chain.astream({\n",
    "        \"length\": \"short\",\n",
    "        \"topic\": \"robots learning to paint\"\n",
    "    }):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# If running in notebook, use this instead of asyncio.run\n",
    "await generate_stream()  # ✅ Works in notebooks or any already-running event loop\n",
    "\n",
    "# 4. Stream with config\n",
    "config = RunnableConfig(\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    run_name=\"Streaming Example\"\n",
    ")\n",
    "\n",
    "chain = ChatPromptTemplate.from_template(\"Explain {topic} in detail\") | chatllm \n",
    "chain.invoke({\"topic\": \"quantum computing\"}, config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ee875-7c9f-4eec-9e3b-45ba85fcf4c0",
   "metadata": {},
   "source": [
    "### Building a Chatbot with LCEL and Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9524a2b-1a98-4b48-be4d-7a1458aba453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AIMessage(content=\"Nice to meet you, Alice! I'm happy to be your helpful AI assistant. Is there something I can assist you with, or would you like to chat for a bit?\", additional_kwargs={}, response_metadata={}, id='run-0376cea7-5d86-451c-8bf8-6b25bae51ca1-0'),\n",
       " AIMessage(content='Your name is Alice!', additional_kwargs={}, response_metadata={}, id='run-cc629b10-0d3d-4b3e-b810-bffa5e4590c0-0'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 1. Create chat template with history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 2. Create LLM\n",
    "model = GroqChatLLM(temperature=0.7, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# 3. Create the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# 4. Add message history\n",
    "message_history = {}  # store histories by session_id\n",
    "\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in message_history:\n",
    "        message_history[session_id] = ChatMessageHistory()\n",
    "    return message_history[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# 5. Run the chatbot\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hello, my name is Alice.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"alice_session\"}}\n",
    ")\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"alice_session\"}}\n",
    ")\n",
    "\n",
    "response1, response2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
